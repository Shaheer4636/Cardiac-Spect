{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "luBdpri6Yrli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d7de136-f8b3-473b-cf44-ce6854c52cab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1594 images belonging to 2 classes.\n",
            "Found 1992 images belonging to 2 classes.\n",
            "Epoch 1/50\n",
            "100/100 [==============================] - 119s 1s/step - loss: 0.6945 - accuracy: 0.4925 - val_loss: 0.6908 - val_accuracy: 0.5070\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 129s 1s/step - loss: 0.6884 - accuracy: 0.5339 - val_loss: 0.6888 - val_accuracy: 0.5291\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 112s 1s/step - loss: 0.6906 - accuracy: 0.5307 - val_loss: 0.6869 - val_accuracy: 0.5587\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 117s 1s/step - loss: 0.6892 - accuracy: 0.5314 - val_loss: 0.6852 - val_accuracy: 0.5698\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 129s 1s/step - loss: 0.6817 - accuracy: 0.5640 - val_loss: 0.6918 - val_accuracy: 0.5126\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 129s 1s/step - loss: 0.6788 - accuracy: 0.5609 - val_loss: 0.6821 - val_accuracy: 0.5828\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 129s 1s/step - loss: 0.6825 - accuracy: 0.5596 - val_loss: 0.6828 - val_accuracy: 0.5658\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 130s 1s/step - loss: 0.6769 - accuracy: 0.5910 - val_loss: 0.6792 - val_accuracy: 0.5788\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 129s 1s/step - loss: 0.6773 - accuracy: 0.5659 - val_loss: 0.6811 - val_accuracy: 0.5783\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 128s 1s/step - loss: 0.6712 - accuracy: 0.5979 - val_loss: 0.6783 - val_accuracy: 0.5763\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 132s 1s/step - loss: 0.6778 - accuracy: 0.5747 - val_loss: 0.6768 - val_accuracy: 0.5833\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 128s 1s/step - loss: 0.6698 - accuracy: 0.5885 - val_loss: 0.6807 - val_accuracy: 0.5788\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.6707 - accuracy: 0.5960 - val_loss: 0.6795 - val_accuracy: 0.5798\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 126s 1s/step - loss: 0.6753 - accuracy: 0.5734 - val_loss: 0.6837 - val_accuracy: 0.5788\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 125s 1s/step - loss: 0.6679 - accuracy: 0.5985 - val_loss: 0.6736 - val_accuracy: 0.5858\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 127s 1s/step - loss: 0.6708 - accuracy: 0.5979 - val_loss: 0.6717 - val_accuracy: 0.5894\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 127s 1s/step - loss: 0.6665 - accuracy: 0.6136 - val_loss: 0.6747 - val_accuracy: 0.5863\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.6674 - accuracy: 0.6023 - val_loss: 0.6736 - val_accuracy: 0.5858\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 127s 1s/step - loss: 0.6652 - accuracy: 0.6010 - val_loss: 0.6709 - val_accuracy: 0.5949\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 127s 1s/step - loss: 0.6633 - accuracy: 0.5903 - val_loss: 0.6721 - val_accuracy: 0.5964\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 127s 1s/step - loss: 0.6668 - accuracy: 0.6060 - val_loss: 0.6729 - val_accuracy: 0.5954\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 127s 1s/step - loss: 0.6584 - accuracy: 0.6186 - val_loss: 0.6804 - val_accuracy: 0.5904\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 127s 1s/step - loss: 0.6650 - accuracy: 0.6004 - val_loss: 0.6700 - val_accuracy: 0.5904\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.6583 - accuracy: 0.6142 - val_loss: 0.6793 - val_accuracy: 0.5894\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 112s 1s/step - loss: 0.6599 - accuracy: 0.6142 - val_loss: 0.6728 - val_accuracy: 0.5899\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 117s 1s/step - loss: 0.6573 - accuracy: 0.6123 - val_loss: 0.6688 - val_accuracy: 0.5939\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 125s 1s/step - loss: 0.6654 - accuracy: 0.5903 - val_loss: 0.6669 - val_accuracy: 0.5853\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 134s 1s/step - loss: 0.6623 - accuracy: 0.6161 - val_loss: 0.6713 - val_accuracy: 0.6004\n",
            "Epoch 29/50\n",
            "100/100 [==============================] - 133s 1s/step - loss: 0.6606 - accuracy: 0.6154 - val_loss: 0.6667 - val_accuracy: 0.6039\n",
            "Epoch 30/50\n",
            "100/100 [==============================] - 128s 1s/step - loss: 0.6538 - accuracy: 0.6142 - val_loss: 0.6704 - val_accuracy: 0.6054\n",
            "Epoch 31/50\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.6584 - accuracy: 0.6142 - val_loss: 0.6651 - val_accuracy: 0.6084\n",
            "Epoch 32/50\n",
            "100/100 [==============================] - 125s 1s/step - loss: 0.6568 - accuracy: 0.6079 - val_loss: 0.6674 - val_accuracy: 0.6039\n",
            "Epoch 33/50\n",
            "100/100 [==============================] - 127s 1s/step - loss: 0.6500 - accuracy: 0.6223 - val_loss: 0.6647 - val_accuracy: 0.6099\n",
            "Epoch 34/50\n",
            "100/100 [==============================] - 129s 1s/step - loss: 0.6511 - accuracy: 0.6223 - val_loss: 0.6606 - val_accuracy: 0.6009\n",
            "Epoch 35/50\n",
            "100/100 [==============================] - 112s 1s/step - loss: 0.6527 - accuracy: 0.6173 - val_loss: 0.6633 - val_accuracy: 0.6130\n",
            "Epoch 36/50\n",
            "100/100 [==============================] - 128s 1s/step - loss: 0.6501 - accuracy: 0.6280 - val_loss: 0.6584 - val_accuracy: 0.6079\n",
            "Epoch 37/50\n",
            "100/100 [==============================] - 115s 1s/step - loss: 0.6560 - accuracy: 0.6230 - val_loss: 0.6579 - val_accuracy: 0.6084\n",
            "Epoch 38/50\n",
            "100/100 [==============================] - 114s 1s/step - loss: 0.6485 - accuracy: 0.6311 - val_loss: 0.6613 - val_accuracy: 0.6160\n",
            "Epoch 39/50\n",
            "100/100 [==============================] - 114s 1s/step - loss: 0.6421 - accuracy: 0.6336 - val_loss: 0.6566 - val_accuracy: 0.6089\n",
            "Epoch 40/50\n",
            "100/100 [==============================] - 130s 1s/step - loss: 0.6519 - accuracy: 0.6066 - val_loss: 0.6553 - val_accuracy: 0.6084\n",
            "Epoch 41/50\n",
            "100/100 [==============================] - 116s 1s/step - loss: 0.6492 - accuracy: 0.6424 - val_loss: 0.6545 - val_accuracy: 0.6135\n",
            "Epoch 42/50\n",
            "100/100 [==============================] - 128s 1s/step - loss: 0.6573 - accuracy: 0.6035 - val_loss: 0.6557 - val_accuracy: 0.6009\n",
            "Epoch 43/50\n",
            "100/100 [==============================] - 114s 1s/step - loss: 0.6472 - accuracy: 0.6261 - val_loss: 0.6543 - val_accuracy: 0.6130\n",
            "Epoch 44/50\n",
            "100/100 [==============================] - 112s 1s/step - loss: 0.6492 - accuracy: 0.6173 - val_loss: 0.6543 - val_accuracy: 0.6130\n",
            "Epoch 45/50\n",
            "100/100 [==============================] - 128s 1s/step - loss: 0.6512 - accuracy: 0.6261 - val_loss: 0.6530 - val_accuracy: 0.6114\n",
            "Epoch 46/50\n",
            "100/100 [==============================] - 128s 1s/step - loss: 0.6448 - accuracy: 0.6274 - val_loss: 0.6567 - val_accuracy: 0.6024\n",
            "Epoch 47/50\n",
            "100/100 [==============================] - 128s 1s/step - loss: 0.6464 - accuracy: 0.6368 - val_loss: 0.6519 - val_accuracy: 0.6130\n",
            "Epoch 48/50\n",
            "100/100 [==============================] - 128s 1s/step - loss: 0.6490 - accuracy: 0.6299 - val_loss: 0.6496 - val_accuracy: 0.6205\n",
            "Epoch 49/50\n",
            "100/100 [==============================] - 133s 1s/step - loss: 0.6465 - accuracy: 0.6299 - val_loss: 0.6577 - val_accuracy: 0.6165\n",
            "Epoch 50/50\n",
            "100/100 [==============================] - 127s 1s/step - loss: 0.6494 - accuracy: 0.6217 - val_loss: 0.6475 - val_accuracy: 0.6210\n",
            "Found 1992 images belonging to 2 classes.\n",
            "125/125 [==============================] - 27s 213ms/step - loss: 0.6475 - accuracy: 0.6210\n",
            "Test Accuracy: 0.6209839582443237\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Set your image dimensions\n",
        "img_height, img_width = 90, 90\n",
        "img_channels = 3  # For RGB images\n",
        "\n",
        "# Set your dataset path within Colab\n",
        "base_path = '/content/'  # Adjust this path\n",
        "\n",
        "# Define paths for training, validation, and testing\n",
        "train_folder = 'Train'\n",
        "valid_folder = 'Valid'\n",
        "test_folder = 'Test'\n",
        "\n",
        "train_path = os.path.join(base_path, train_folder)\n",
        "valid_path = os.path.join(base_path, valid_folder)\n",
        "test_path = os.path.join(base_path, test_folder)\n",
        "\n",
        "# Specify class names\n",
        "class_names = ['Abnormal', 'Normal']\n",
        "\n",
        "# Train data generator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    classes=class_names,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Validation data generator\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    valid_path,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    classes=class_names\n",
        ")\n",
        "\n",
        "# Define your custom model\n",
        "custom_model = models.Sequential()\n",
        "custom_model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(img_height, img_width, img_channels)))\n",
        "custom_model.add(layers.MaxPooling2D((2, 2)))\n",
        "custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "custom_model.add(layers.MaxPooling2D((2, 2)))\n",
        "custom_model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "custom_model.add(layers.MaxPooling2D((2, 2)))\n",
        "custom_model.add(layers.Flatten())\n",
        "custom_model.add(layers.Dense(512, activation='relu'))\n",
        "custom_model.add(layers.Dropout(0.5))\n",
        "\n",
        "# Load InceptionV3 (GoogLeNet) model with pre-trained weights (excluding the top layers)\n",
        "inception_model = InceptionV3(input_shape=(img_height, img_width, img_channels), include_top=False, weights='imagenet')\n",
        "\n",
        "# Fine-tune InceptionV3 layers\n",
        "for layer in inception_model.layers[:249]:\n",
        "    layer.trainable = False\n",
        "for layer in inception_model.layers[249:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Create an input tensor with the same shape as the InceptionV3 model's input\n",
        "inception_input = layers.Input(shape=(img_height, img_width, img_channels))\n",
        "\n",
        "# Apply InceptionV3 model to the input tensor\n",
        "inception_output = inception_model(inception_input)\n",
        "\n",
        "# Add GlobalAveragePooling2D layer to reduce spatial dimensions\n",
        "inception_output = layers.GlobalAveragePooling2D()(inception_output)\n",
        "\n",
        "# Combine the custom model and InceptionV3 model\n",
        "combined_model = models.Sequential()\n",
        "combined_model.add(custom_model)\n",
        "combined_model.add(layers.Dense(512, activation='relu'))\n",
        "combined_model.add(layers.Dropout(0.5))\n",
        "combined_model.add(layers.Dense(len(class_names), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "combined_model.compile(optimizer=optimizers.Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = combined_model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Assuming you have a 'Test' folder for evaluation\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    classes=class_names\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = combined_model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {test_accuracy}')\n"
      ]
    }
  ]
}